{
	"URL": "https://arxiv.org/abs/1502.05477",
  "method_name": "PPO",
  "full_name": "Proximal Policy Optimization",
  "category": "reinforcement_learning",
  "base_algorithm": {
    "derived_from": "TRPO (Trust Region Policy Optimization)",
    "key_modification": "使用 clipped probability ratio 的代理目标函数替代 TRPO 的硬约束，形成策略性能的悲观下界估计；支持多轮 minibatch 更新；仅需一阶优化"
  },
  "data_source": {
    "sampling_strategy": "online"
  },
  "reward_function": {
    "type": null,
    "granularity": null
  },
  "gradient_coefficient": {
    "clipping_mechanism": "使用 clip(r_t(θ), 1-ε, 1+ε) 限制 probability ratio，ε 为超参数（如 0.2）；取 clipped 和 unclipped 目标的最小值，形成悲观下界",
    "advantage_estimation": "使用 Generalized Advantage Estimation (GAE)，δ_t = r_t + γV(s_{t+1}) - V(s_t)，Â_t 为 TD 残差的加权和",
    "combined_objective": "L^{CLIP+VF+S}(θ) = E_t[L^{CLIP}_t(θ) - c_1 L^{VF}_t(θ) + c_2 S[π_θ](s_t)]，其中 c_1、c_2 为系数"
  },
  "training_paradigm": {
    "iterative": true,
    "reference_model_update": "每次策略更新后，θ_old ← θ",
    "reward_model_update": null
  },
  "regularization": {
    "type": "clipped surrogate objective 或 adaptive KL penalty",
    "clipping_parameter": "ε (如 0.1, 0.2, 0.3)",
    "kl_penalty": {
      "adaptive": "根据实际 KL 散度与目标 d_targ 的比较，动态调整 β：若 d < d_targ/1.5 则 β ← β/2；若 d > d_targ×1.5 则 β ← β×2",
      "fixed": "使用固定 β 值（如 0.3, 1, 3, 10）"
    },
    "entropy_bonus": "S[π_θ](s_t) 熵奖励项，系数 c_2（Atari 实验中 c_2=0.01）"
  },
  "efficiency": {
    "value_model": true,
    "parallel_actors": "N 个并行 actor（MuJoCo: 未明确指定; Roboschool: 32或128; Atari: 8）",
    "trajectory_length": "T timesteps（MuJoCo: 2048; Roboschool: 512; Atari: 128）",
    "minibatch_size": "M ≤ NT（MuJoCo: 64; Roboschool: 4096; Atari: 256）",
    "optimization_epochs": "K epochs（MuJoCo: 10; Roboschool: 15; Atari: 3）",
    "optimizer": "Adam"
  },
  "empirical_results": {
    "in_domain": {
      "continuous_control": {
        "benchmark": "7 MuJoCo simulated robotics tasks (OpenAI Gym)",
        "timesteps": "1 million",
        "best_clipping_epsilon": "ε=0.2，avg normalized score 0.82",
        "comparison": "PPO 在几乎所有连续控制环境上优于 TRPO、CEM、vanilla policy gradient、A2C、A2C with trust region"
      },
      "humanoid_tasks": {
        "tasks": "RoboschoolHumanoid, RoboschoolHumanoidFlagrun, RoboschoolHumanoidFlagrunHarder",
        "description": "3D humanoid 跑步、转向、起身等高维连续控制任务"
      }
    },
    "out_of_domain": {
      "atari": {
        "benchmark": "Arcade Learning Environment, 49 games",
        "comparison_with_A2C": "整个训练期间平均奖励：PPO 赢 30 局，A2C 赢 1 局；最后 100 episode 平均奖励：PPO 赢 19 局，ACER 赢 28 局",
        "note": "PPO 在样本复杂度上显著优于 A2C，与 ACER 表现相近但实现更简单"
      }
    }
  },
  "theoretical_insight": {
    "surrogate_objective": "L^{CLIP} 是 L^{CPI} 的悲观下界（lower bound），通过 clipping 惩罚过大的策略更新",
    "monotonic_improvement": "目标函数设计使得仅在改进目标时忽略 ratio 变化，在恶化目标时保留，从而实现类似 TRPO 的单调改进特性",
    "first_order_optimization": "相比 TRPO 使用共轭梯度的二阶方法，PPO 仅需一阶优化（SGD/Adam），实现更简单且适用于更通用的架构（如参数共享、dropout）"
  }
}