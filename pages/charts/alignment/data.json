[
  {
    "id": "instructgpt",
    "name": "InstructGPT",
    "date": "2022-03-01",
    "value": 3.0,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "SFT + RM + PPO",
    "result": "确立了 RLHF 范式，1.3B 效果优于 175B GPT-3",
    "paper": "Training language models to follow instructions"
  },
  {
    "id": "cai_claude1",
    "name": "Claude 1",
    "date": "2022-12-15",
    "value": 4.5,
    "cluster": "RLAIF (AI反馈)",
    "domain": "general",
    "conditions": "Constitutional AI (SL-CAI + RL-CAI)",
    "result": "Anthropic 首秀，证明了用原则（Constitution）而非人类标签也能对齐",
    "paper": "Constitutional AI: Harmlessness from AI Feedback"
  },
  {
    "id": "llama1",
    "name": "Llama 1",
    "date": "2023-02-24",
    "value": 3.5,
    "cluster": "SFT / Distillation",
    "domain": "general",
    "conditions": "Pre-training Only (Base Model)",
    "result": "开源大模型的基石，引发了后续的微调狂潮",
    "paper": "LLaMA: Open and Efficient Foundation Language Models"
  },
  {
    "id": "chatglm_6b",
    "name": "ChatGLM-6B",
    "date": "2023-03-14",
    "value": 3.9,
    "cluster": "SFT / Distillation",
    "domain": "general",
    "conditions": "GLM Arch + P-Tuning",
    "result": "低显存部署先驱，让消费级显卡跑大模型在中国普及",
    "paper": "GLM-130B: An Open Bilingual Pre-trained Model"
  },
  {
    "id": "gpt4",
    "name": "GPT-4",
    "date": "2023-03-14",
    "value": 6.8,
    "cluster": "RLHF (传统派)",
    "domain": "reasoning",
    "conditions": "Large-scale RLHF + Rule-based RM + MoE",
    "result": "设定了行业天花板，逻辑推理能力在此后的一年内无人能敌",
    "paper": "GPT-4 Technical Report",
    "experimentDetails": {
      "hardware": "GPU集群",
      "gpuCount": 10000,
      "gpuModel": "A100",
      "memory": "PB级",
      "trainingTime": "数月",
      "totalHours": "约10000小时",
      "tokens": "13T tokens",
      "dataSize": "13万亿tokens",
      "modelSize": "~1.7T (MoE)",
      "learningRate": "1e-5",
      "batchSize": "4096",
      "cost": "约$100M"
    },
    "metrics": {
      "benchmarks": {
        "MMLU": "86.4%",
        "HellaSwag": "95.3%",
        "HumanEval": "67.0%",
        "GSM8K": "92.0%",
        "MATH": "53.9%"
      },
      "latency": "~200ms",
      "throughput": "~50 tokens/s",
      "tokensPerDollar": "~2.5M tokens/$"
    }
  },
  {
    "id": "vicuna",
    "name": "Vicuna",
    "date": "2023-03-30",
    "value": 4.2,
    "cluster": "SFT / Distillation",
    "domain": "general",
    "conditions": "ShareGPT Dialogue Fine-tuning",
    "result": "LMSYS 出品，在当时达到了 ChatGPT 90% 的体感质量",
    "paper": "Vicuna: An Open-Source Chatbot"
  },
  {
    "id": "wizardlm",
    "name": "WizardLM",
    "date": "2023-04-24",
    "value": 4.5,
    "cluster": "SFT / Distillation",
    "domain": "reasoning",
    "conditions": "Evol-Instruct (指令进化)",
    "result": "提出自动化提升数据复杂度的算法，极大增强了小模型的逻辑能力",
    "paper": "WizardLM: Empowering LLMs to Follow Complex Instructions"
  },
  {
    "id": "dpo_paper",
    "name": "DPO (Paper)",
    "date": "2023-05-29",
    "value": 7.0,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "No RL, Classification Loss",
    "result": "提出 DPO 算法，证明不需要 RM 也能对齐，彻底改变了开源对齐格局",
    "paper": "Direct Preference Optimization"
  },
  {
    "id": "prm_math_shepherd",
    "name": "PRM (Math Shepherd)",
    "date": "2023-05-31",
    "value": 6.8,
    "cluster": "PRM (过程监督)",
    "domain": "reasoning",
    "conditions": "Process Reward Model (步骤级奖励)",
    "result": "验证了过程监督比结果监督更能减少逻辑谬误",
    "paper": "Let's Verify Step by Step"
  },
  {
    "id": "orca",
    "name": "Microsoft Orca",
    "date": "2023-06-05",
    "value": 5.2,
    "cluster": "Reasoning / RLVF",
    "domain": "reasoning",
    "conditions": "Explanation Tuning (学习推理过程)",
    "result": "不仅模仿 GPT-4 的答案，还模仿其'思考过程'（System Instructions）",
    "paper": "Orca: Progressive Learning from Complex Explanation Traces"
  },
  {
    "id": "internlm_7b",
    "name": "InternLM-7B",
    "date": "2023-06-07",
    "value": 4.7,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "Polylingual RLHF",
    "result": "上海 AI Lab 力作，建立了完整的书生·浦语开源体系",
    "paper": "InternLM: A Multilingual Language Model"
  },
  {
    "id": "baichuan_13b",
    "name": "Baichuan-13B",
    "date": "2023-07-11",
    "value": 5.0,
    "cluster": "SFT / Distillation",
    "domain": "general",
    "conditions": "High Quality Chinese Corpus",
    "result": "百川智能发布，中文语境下表现极佳的早期开源模型",
    "paper": "Baichuan-13B Technical Report"
  },
  {
    "id": "claude2",
    "name": "Claude 2",
    "date": "2023-07-11",
    "value": 6.2,
    "cluster": "RLAIF (AI反馈)",
    "domain": "general",
    "conditions": "Massive RLAIF + Long Context (100k)",
    "result": "长文本与安全性的典范，成为 GPT-4 的强力竞争者",
    "paper": "Model Card and Evaluations for Claude Models"
  },
  {
    "id": "llama2",
    "name": "Llama 2 Chat",
    "date": "2023-07-18",
    "value": 5.8,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "RLHF (PPO) + Rejection Sampling",
    "result": "定义了 2023 年下半年的开源标准，PPO 对齐的教科书案例",
    "paper": "Llama 2: Open Foundation and Chat Models"
  },
  {
    "id": "codellama",
    "name": "Code Llama",
    "date": "2023-08-24",
    "value": 6.0,
    "cluster": "Reasoning / RLVF",
    "domain": "reasoning",
    "conditions": "In-filling + Long Context + RLEF",
    "result": "专攻代码能力的开源模型，填补了 Llama 2 在编程上的短板",
    "paper": "Code Llama: Open Foundation Models for Code"
  },
  {
    "id": "hunyuan_pro",
    "name": "Hunyuan-Pro (腾讯混元)",
    "date": "2023-09-07",
    "value": 6.3,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "MoE + RLHF",
    "result": "腾讯推出的万亿参数 MoE 模型，中文综合能力第一梯队",
    "paper": "Tencent Hunyuan Technical Report"
  },
  {
    "id": "mistral_7b",
    "name": "Mistral 7B",
    "date": "2023-09-27",
    "value": 6.4,
    "cluster": "SFT / Distillation",
    "domain": "general",
    "conditions": "Sliding Window Attention (SWA)",
    "result": "欧洲之光，7B 参数打败 Llama 2 13B，架构极其高效",
    "paper": "Mistral 7B"
  },
  {
    "id": "zephyr_beta",
    "name": "Zephyr-7B-β",
    "date": "2023-10-25",
    "value": 7.2,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "dDPO (Distilled DPO) on Mistral",
    "result": "HuggingFaceH4 出品，首个证明 DPO 在实战中超越 PPO 的小模型",
    "paper": "Zephyr: Direct Distillation of LM Alignment"
  },
  {
    "id": "deepseek_coder",
    "name": "DeepSeek Coder",
    "date": "2023-11-01",
    "value": 6.3,
    "cluster": "Reasoning / RLVF",
    "domain": "reasoning",
    "conditions": "Fill-in-the-middle + Unit Test Feedback",
    "result": "早期代码生成领域的开源黑马，代码库训练数据量巨大",
    "paper": "DeepSeek Coder: When the Large Language Model Meets Programming"
  },
  {
    "id": "grok1",
    "name": "Grok-1",
    "date": "2023-11-04",
    "value": 5.5,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "RLHF with real-time access + Jarviz",
    "result": "xAI 的首秀，强调实时性和幽默感（Fun Mode）",
    "paper": "Announcing Grok"
  },
  {
    "id": "yi_34b",
    "name": "Yi-34B",
    "date": "2023-11-06",
    "value": 6.9,
    "cluster": "SFT / Distillation",
    "domain": "general",
    "conditions": "High Quality Pre-training Data (3T tokens)",
    "result": "零一万物发布，凭借强悍的基座能力长时间霸榜 HuggingFace",
    "paper": "The Yi Series Models Report"
  },
  {
    "id": "starling",
    "name": "Starling-7B",
    "date": "2023-11-20",
    "value": 6.7,
    "cluster": "RLAIF (AI反馈)",
    "domain": "general",
    "conditions": "RAILF (Ranking via AI) + PPO",
    "result": "利用 GPT-4 打分构建 Reward Model 的代表作，Nectar 数据集来源",
    "paper": "Starling-7B: Increasing LLM Helpfulness with RLAIF"
  },
  {
    "id": "mixtral",
    "name": "Mixtral 8x7B",
    "date": "2023-12-11",
    "value": 7.6,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Sparse MoE + DPO",
    "result": "MoE 架构的成功应用，推理效率极高，开源界首个 GPT-3.5 杀手",
    "paper": "Mixtral of Experts"
  },
  {
    "id": "gemini1_ultra",
    "name": "Gemini 1.0 Ultra",
    "date": "2023-12-06",
    "value": 7.1,
    "cluster": "RLHF (传统派)",
    "domain": "reasoning",
    "conditions": "Native Multimodal + RLHF",
    "result": "Google 的反击，原生多模态架构，MMLU 首次超越人类专家",
    "paper": "Gemini: A Family of Highly Capable Multimodal Models"
  },
  {
    "id": "ernie_4",
    "name": "Ernie 4.0 (文心一言)",
    "date": "2023-10-17",
    "value": 7.5,
    "cluster": "RLHF (传统派)",
    "domain": "reasoning",
    "conditions": "Knowledge Enhanced + PPO",
    "result": "百度最强模型，中文知识图谱增强，逻辑推理能力显著提升",
    "paper": "Ernie Bot 4.0 Launch"
  },
  {
    "id": "glm4",
    "name": "GLM-4",
    "date": "2024-01-16",
    "value": 8.0,
    "cluster": "RLHF (传统派)",
    "domain": "reasoning",
    "conditions": "All Tools Integration (GLM Agent)",
    "result": "智谱 AI 的全能模型，Agent 能力和工具调用能力突出",
    "paper": "GLM-4 Report"
  },
  {
    "id": "internlm2_math",
    "name": "InternLM2-Math",
    "date": "2024-01-30",
    "value": 7.5,
    "cluster": "PRM (过程监督)",
    "domain": "reasoning",
    "conditions": "MinHash Deduplication + RLVF + PRM",
    "result": "Math-Shepherd 的实践者，将数学推理作为对齐核心",
    "paper": "InternLM2 Technical Report"
  },
  {
    "id": "kto_paper",
    "name": "KTO (Paper)",
    "date": "2024-02-01",
    "value": 7.8,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Binary Feedback (Thumbs up/down)",
    "result": "证明不需要成对数据，利用弱反馈也能对齐，更适合大规模数据",
    "paper": "KTO: Model Alignment as Prospect Theoretic Optimization"
  },
  {
    "id": "qwen1_5",
    "name": "Qwen 1.5",
    "date": "2024-02-04",
    "value": 7.7,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "DPO / PPO Hybrid",
    "result": "通义千问系列开始霸榜开源榜单，包含 0.5B 到 72B 全系列",
    "paper": "Qwen Technical Report"
  },
  {
    "id": "deepseek_math",
    "name": "DeepSeek Math",
    "date": "2024-02-05",
    "value": 8.1,
    "cluster": "Reasoning / RLVF",
    "domain": "reasoning",
    "conditions": "GRPO (Group Relative Policy Opt)",
    "result": "GRPO 算法首秀，7B 模型在数学上击败 GPT-4，R1 的前身",
    "paper": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning"
  },
  {
    "id": "gemini15_pro",
    "name": "Gemini 1.5 Pro",
    "date": "2024-02-15",
    "value": 8.6,
    "cluster": "RLHF (传统派)",
    "domain": "reasoning",
    "conditions": "1M+ Context Window + RLHF",
    "result": "突破了长上下文对齐的极限，支持大海捞针（NIAH）",
    "paper": "Gemini 1.5: Unlocking Multimodal Understanding"
  },
  {
    "id": "gemma",
    "name": "Gemma 1",
    "date": "2024-02-21",
    "value": 6.8,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "RLHF on TPU",
    "result": "Google 开放权重的轻量级模型，基于 Gemini 技术栈",
    "paper": "Gemma: Open Models Based on Gemini Research"
  },
  {
    "id": "claude3_opus",
    "name": "Claude 3 Opus",
    "date": "2024-03-04",
    "value": 8.8,
    "cluster": "RLAIF (AI反馈)",
    "domain": "reasoning",
    "conditions": "Advanced RLAIF + Constitutional",
    "result": "首个在综合基准测试上超越 GPT-4 的模型，确立了 Anthropic 的顶级地位",
    "paper": "The Claude 3 Model Family"
  },
  {
    "id": "command_r",
    "name": "Command R",
    "date": "2024-03-11",
    "value": 7.4,
    "cluster": "SFT / Distillation",
    "domain": "reasoning",
    "conditions": "RAG-optimized SFT",
    "result": "Cohere 发布的专为 RAG 检索增强生成的模型",
    "paper": "Command R Blog"
  },
  {
    "id": "kimi_moonshot",
    "name": "Kimi (Moonshot)",
    "date": "2024-03-18",
    "value": 8.0,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "Lossless Long Context (200k+)",
    "result": "月之暗面，引爆了长文本对齐的商业竞争，用户留存极高",
    "paper": "Moonshot AI Blog"
  },
  {
    "id": "step_1",
    "name": "Step-1 (阶跃星辰)",
    "date": "2024-03-20",
    "value": 8.2,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "Multimodal RLHF",
    "result": "中国创业公司阶跃星辰的万亿参数模型，多模态能力出色",
    "paper": "Step-1 Launch"
  },
  {
    "id": "dbrx",
    "name": "Databricks DBRX",
    "date": "2024-03-27",
    "value": 7.6,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Fine-grained MoE + Policy Opt",
    "result": "企业级 MoE 模型的代表，超越了 Mixtral 和 Llama 2",
    "paper": "Inside the Creation of DBRX"
  },
  {
    "id": "command_r_plus",
    "name": "Command R+",
    "date": "2024-04-04",
    "value": 7.9,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "Tool-use Optimization + RAG",
    "result": "在 RAG 和复杂 Agent 任务上表现卓越的开源模型",
    "paper": "Cohere Command R+"
  },
  {
    "id": "abab_65",
    "name": "Abab 6.5",
    "date": "2024-04-17",
    "value": 8.1,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "reasoning",
    "conditions": "MoE + DPO + Linear Attn",
    "result": "MiniMax 的 MoE 模型，在处理超长文本上有独特优势",
    "paper": "MiniMax Abab 6.5"
  },
  {
    "id": "llama3",
    "name": "Llama 3",
    "date": "2024-04-18",
    "value": 8.9,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "High Quality Data + PPO/DPO",
    "result": "8B 模型性能极其强悍，确立 DPO 为社区主流微调手段",
    "paper": "The Llama 3 Flock of Models"
  },
  {
    "id": "phi3",
    "name": "Phi-3",
    "date": "2024-04-23",
    "value": 7.5,
    "cluster": "SFT / Distillation",
    "domain": "reasoning",
    "conditions": "High Quality Synthetic Data",
    "result": "3.8B 参数在推理能力上对标 Mixtral，手机端可跑",
    "paper": "Phi-3 Technical Report"
  },
  {
    "id": "deepseek_v2",
    "name": "DeepSeek-V2",
    "date": "2024-05-06",
    "value": 8.7,
    "cluster": "Reasoning / RLVF",
    "domain": "general",
    "conditions": "MLA (Attention) + GRPO + MoE",
    "result": "架构创新(MLA)配合 GRPO，把 GPT-4 级模型价格打下来的功臣",
    "paper": "DeepSeek-V2 Technical Report"
  },
  {
    "id": "gpt4o",
    "name": "GPT-4o",
    "date": "2024-05-13",
    "value": 9.2,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "End-to-End Multimodal RLHF",
    "result": "全能模型，延迟极低，语音/视觉/文本统一对齐，人机交互新标杆",
    "paper": "Hello GPT-4o",
    "experimentDetails": {
      "hardware": "超大规模GPU集群",
      "gpuCount": 12000,
      "gpuModel": "H100",
      "trainingTime": "数月",
      "totalHours": "约12000小时",
      "tokens": "18T tokens",
      "dataSize": "18万亿tokens (多模态)",
      "modelSize": "~1.8T (MoE)",
      "learningRate": "8e-6",
      "batchSize": "8192",
      "cost": "约$200M"
    },
    "metrics": {
      "benchmarks": {
        "MMLU": "88.7%",
        "HumanEval": "90.2%",
        "GSM8K": "97.0%",
        "MATH": "76.2%",
        "Multimodal": "SOTA"
      },
      "latency": "~80ms",
      "throughput": "~120 tokens/s",
      "tokensPerDollar": "~4.5M tokens/$"
    }
  },
  {
    "id": "doubao_pro",
    "name": "Doubao-Pro (豆包)",
    "date": "2024-05-15",
    "value": 8.8,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "Large Scale RLHF + Audio",
    "result": "字节跳动主力模型，日活极高，语音交互与文本对齐并重",
    "paper": "Doubao Launch"
  },
  {
    "id": "simpo_paper",
    "name": "SimPO (Paper)",
    "date": "2024-05-23",
    "value": 8.3,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "No Reference Model Needed",
    "result": "比 DPO 更简单，显存占用更低，被 Llama 3 后续微调广泛采用",
    "paper": "SimPO: Simple Preference Optimization"
  },
  {
    "id": "apple_afm",
    "name": "Apple AFM",
    "date": "2024-06-10",
    "value": 7.8,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "RLHF + LoRA Adapters (On-device)",
    "result": "苹果智能核心，强调隐私保护与特定功能的适配器切换",
    "paper": "Apple Intelligence Foundation Language Models"
  },
  {
    "id": "nemotron_4_reward",
    "name": "Nemotron-4 Reward",
    "date": "2024-06-14",
    "value": 8.6,
    "cluster": "RLAIF (AI反馈)",
    "domain": "reasoning",
    "conditions": "Synthetic Data Generation + Reward Model",
    "result": "NVIDIA 发布的最强开源 RM，霸榜 HuggingFace，专门用于生成合成数据闭环",
    "paper": "Nemotron-4 340B Technical Report"
  },
  {
    "id": "claude35_sonnet",
    "name": "Claude 3.5 Sonnet",
    "date": "2024-06-20",
    "value": 9.4,
    "cluster": "RLAIF (AI反馈)",
    "domain": "reasoning",
    "conditions": "Refined RLAIF + Artifacts UI",
    "result": "体感最强模型，代码和逻辑能力全面超越 GPT-4o，Artifacts 改变交互",
    "paper": "Claude 3.5 Sonnet",
    "experimentDetails": {
      "hardware": "大规模GPU集群",
      "gpuCount": 8000,
      "gpuModel": "H100",
      "trainingTime": "数月",
      "totalHours": "约8000小时",
      "tokens": "15T tokens",
      "dataSize": "15万亿tokens",
      "modelSize": "~200B",
      "learningRate": "2e-5",
      "batchSize": "6144",
      "cost": "约$150M"
    },
    "metrics": {
      "benchmarks": {
        "MMLU": "92.0%",
        "HumanEval": "84.9%",
        "GSM8K": "96.0%",
        "MATH": "71.2%",
        "Arena Hard": "Top 1"
      },
      "latency": "~150ms",
      "throughput": "~60 tokens/s",
      "tokensPerDollar": "~3.2M tokens/$"
    }
  },
  {
    "id": "gemma2",
    "name": "Gemma 2",
    "date": "2024-06-27",
    "value": 8.5,
    "cluster": "SFT / Distillation",
    "domain": "general",
    "conditions": "Knowledge Distillation (27B -> 9B)",
    "result": "证明了蒸馏在 post-training 阶段的威力，9B 模型越级挑战",
    "paper": "Gemma 2 Technical Report"
  },
  {
    "id": "internvl_2",
    "name": "InternVL 2",
    "date": "2024-07-04",
    "value": 8.8,
    "cluster": "SFT / Distillation",
    "domain": "reasoning",
    "conditions": "ViT-InternViT Alignment",
    "result": "开源界多模态能力的领跑者，文档理解能力极强",
    "paper": "InternVL 2 Technical Report"
  },
  {
    "id": "llama31_405b",
    "name": "Llama 3.1 405B",
    "date": "2024-07-23",
    "value": 9.3,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Iterative DPO + SimPO/IPO mix",
    "result": "开源模型首次在参数量和能力上完全对标 GPT-4o，开源里程碑",
    "paper": "The Llama 3 Herd of Models"
  },
  {
    "id": "mistral_large2",
    "name": "Mistral Large 2",
    "date": "2024-07-24",
    "value": 9.0,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Code-focused Alignment + 128k context",
    "result": "欧洲最强闭源模型，参数量适中(123B)但性能顶尖，编码能力突出",
    "paper": "Mistral Large 2 Blog"
  },
  {
    "id": "qwen2_math",
    "name": "Qwen 2 Math",
    "date": "2024-08-08",
    "value": 8.9,
    "cluster": "PRM (过程监督)",
    "domain": "reasoning",
    "conditions": "Specialized Math SFT + PRM + ORPO",
    "result": "专精数学模型，结合了 PRM 技术，Math 榜单开源第一",
    "paper": "Qwen2-Math Technical Report"
  },
  {
    "id": "grok2",
    "name": "Grok-2",
    "date": "2024-08-14",
    "value": 9.0,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "Flux integration + RLHF",
    "result": "进入第一梯队，LMSYS 榜单前列，生成图片不受限",
    "paper": "Grok-2 Blog"
  },
  {
    "id": "phi35_vision",
    "name": "Phi-3.5 Vision",
    "date": "2024-08-20",
    "value": 8.2,
    "cluster": "SFT / Distillation",
    "domain": "reasoning",
    "conditions": "Multimodal Synthetic Data",
    "result": "微软证明了小参数模型也能具备极强的视觉推理能力",
    "paper": "Phi-3.5 Technical Report"
  },
  {
    "id": "solar_pro",
    "name": "Solar Pro",
    "date": "2024-09-01",
    "value": 8.5,
    "cluster": "SFT / Distillation",
    "domain": "general",
    "conditions": "Depth Up-Scaling (DUS)",
    "result": "韩国 Upstage 发布的 22B 模型，单卡性能极强",
    "paper": "Solar Pro Preview"
  },
  {
    "id": "deepseek_v25",
    "name": "DeepSeek-V2.5",
    "date": "2024-09-06",
    "value": 9.1,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Merge of Chat and Coder + DPO",
    "result": "验证了通用能力与代码能力可以完美融合，R1 的基石",
    "paper": "DeepSeek Update"
  },
  {
    "id": "pixtral_12b",
    "name": "Pixtral 12B",
    "date": "2024-09-11",
    "value": 8.6,
    "cluster": "SFT / Distillation",
    "domain": "general",
    "conditions": "Vision Encoder + Mistral Nemo",
    "result": "Mistral 首个多模态模型，图文交错理解能力强",
    "paper": "Pixtral Blog"
  },
  {
    "id": "o1_preview",
    "name": "OpenAI o1-preview",
    "date": "2024-09-12",
    "value": 9.7,
    "cluster": "Reasoning / RLVF",
    "domain": "reasoning",
    "conditions": "CoT + Process Reward + RL Search (Strawberry)",
    "result": "引入'System 2'思维，在奥数/编程题上碾压传统模型，开启推理新时代",
    "paper": "Learning to Reason with LLMs",
    "experimentDetails": {
      "hardware": "大规模GPU集群",
      "gpuCount": 10000,
      "gpuModel": "H100",
      "trainingTime": "数月",
      "totalHours": "约15000小时",
      "tokens": "20T tokens",
      "dataSize": "20万亿tokens (推理数据)",
      "modelSize": "~70B",
      "learningRate": "3e-5",
      "batchSize": "2048",
      "cost": "约$180M"
    },
    "metrics": {
      "benchmarks": {
        "MATH": "94.2%",
        "HumanEval": "91.5%",
        "IMO": "62.5%",
        "USACO": "Gold Level",
        "AIME": "Top 1%"
      },
      "latency": "~5-15s (推理时间)",
      "throughput": "~8 tokens/s",
      "reasoningSteps": "平均15-30步"
    }
  },
  {
    "id": "o1_mini",
    "name": "OpenAI o1-mini",
    "date": "2024-09-12",
    "value": 9.3,
    "cluster": "Reasoning / RLVF",
    "domain": "reasoning",
    "conditions": "Efficient Reasoning Alignment (STEM focused)",
    "result": "针对 STEM 优化的轻量级推理模型，推理速度更快",
    "paper": "OpenAI o1-mini System Card",
    "experimentDetails": {
      "hardware": "GPU集群",
      "gpuCount": 5000,
      "gpuModel": "H100",
      "trainingTime": "数月",
      "totalHours": "约8000小时",
      "tokens": "15T tokens",
      "dataSize": "15万亿tokens (STEM)",
      "modelSize": "~30B",
      "learningRate": "5e-5",
      "batchSize": "2048",
      "cost": "约$80M"
    },
    "metrics": {
      "benchmarks": {
        "MATH": "89.5%",
        "HumanEval": "85.2%",
        "GSM8K": "95.8%",
        "AIME": "Top 5%"
      },
      "latency": "~3-8s (推理时间)",
      "throughput": "~12 tokens/s",
      "reasoningSteps": "平均10-20步"
    }
  },
  {
    "id": "qwen25",
    "name": "Qwen 2.5",
    "date": "2024-09-19",
    "value": 9.2,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Massive scale synthetic data + DPO",
    "result": "2024 下半年最强开源基座，72B 性能逼近 Llama 3.1 405B",
    "paper": "Qwen2.5 Blog"
  },
  {
    "id": "llama32_vision",
    "name": "Llama 3.2 Vision",
    "date": "2024-09-25",
    "value": 9.0,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Vision Adapters + DPO",
    "result": "Meta 的首个开源多模态模型 (11B/90B)，图像理解能力强",
    "paper": "Llama 3.2: Connecting Vision and Edge"
  },
  {
    "id": "molmo",
    "name": "Molmo",
    "date": "2024-09-25",
    "value": 8.8,
    "cluster": "SFT / Distillation",
    "domain": "general",
    "conditions": "Native Pixel-based Alignment",
    "result": "Ai2 发布，不依赖 CLIP，直接像素级对齐，视觉指向(Pointing)能力极强",
    "paper": "Molmo: Open Multimodal Models"
  },
  {
    "id": "nvlm_1",
    "name": "NVLM 1.0",
    "date": "2024-10-01",
    "value": 9.0,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "Decoder-only Multimodal LLM",
    "result": "NVIDIA 开源多模态模型，OCR 和图表理解能力顶级",
    "paper": "NVLM: Open Frontier-Class Multimodal LLMs"
  },
  {
    "id": "yi_lightning",
    "name": "Yi-Lightning",
    "date": "2024-10-10",
    "value": 9.1,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Mixed Precision Training + DPO",
    "result": "01.AI 的竞速模型，在 LMSYS 上排名极高，强调推理速度与质量平衡",
    "paper": "Yi-Lightning Blog"
  },
  {
    "id": "nemotron_70b",
    "name": "Llama-3.1-Nemotron-70B",
    "date": "2024-10-15",
    "value": 9.3,
    "cluster": "RLAIF (AI反馈)",
    "domain": "general",
    "conditions": "RLHF (Reward Model) + SteerLM",
    "result": "NVIDIA 发布的强力对齐版 Llama，在 Arena Hard 上击败 GPT-4o",
    "paper": "Nemotron-70B Model Card"
  },
  {
    "id": "ministral",
    "name": "Ministral 8B",
    "date": "2024-10-16",
    "value": 8.5,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Sliding Window Attention Edge",
    "result": "Mistral AI 针对边缘计算的极致优化",
    "paper": "Les Ministraux"
  },
  {
    "id": "claude35_haiku",
    "name": "Claude 3.5 Haiku",
    "date": "2024-10-22",
    "value": 8.9,
    "cluster": "RLAIF (AI反馈)",
    "domain": "general",
    "conditions": "Distilled from Sonnet + RLAIF",
    "result": "小模型性能之王，甚至超越了上一代的 Opus，纯文本推理极快",
    "paper": "Claude 3.5 Haiku Launch"
  },
  {
    "id": "qwen_qwq",
    "name": "Qwen-QwQ",
    "date": "2024-11-01",
    "value": 9.5,
    "cluster": "Reasoning / RLVF",
    "domain": "reasoning",
    "conditions": "RLVF + Search (Similar to o1)",
    "result": "Qwen 对 o1 路线的探索版，展示了开源界追赶 System 2 的速度",
    "paper": "Qwen Team Blog"
  },
  {
    "id": "hunyuan_large",
    "name": "Hunyuan-Large",
    "date": "2024-11-05",
    "value": 9.2,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "MoE + DPO + Long Context",
    "result": "腾讯开源的万亿参数 MoE，中文语境下对标 Llama 405B",
    "paper": "Hunyuan-Large Technical Report"
  },
  {
    "id": "llama33",
    "name": "Llama 3.3 70B",
    "date": "2024-12-06",
    "value": 9.4,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Distilled from 405B + Online DPO",
    "result": "Meta 年末大招，70B 性能几乎等同于 405B，性价比极高",
    "paper": "Llama 3.3 Blog"
  },
  {
    "id": "gemini2_flash",
    "name": "Gemini 2.0 Flash",
    "date": "2024-12-11",
    "value": 9.6,
    "cluster": "RLHF (传统派)",
    "domain": "reasoning",
    "conditions": "Next-Gen Multimodal RLHF",
    "result": "Google 新一代模型，多模态实时交互延迟极低，逻辑能力增强",
    "paper": "Gemini 2.0 Announcement"
  },
  {
    "id": "deepseek_v3",
    "name": "DeepSeek-V3",
    "date": "2024-12-26",
    "value": 9.6,
    "cluster": "Reasoning / RLVF",
    "domain": "general",
    "conditions": "Auxiliary-loss-free Load Balancing + FP8",
    "result": "训练成本极低($5.5M)的 SOTA 基座，R1 的基础，性能对标 GPT-4o",
    "paper": "DeepSeek-V3 Technical Report"
  },
  {
    "id": "minimax_01",
    "name": "MiniMax-01",
    "date": "2025-01-16",
    "value": 9.2,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Lightning Attention + DPO",
    "result": "长文本处理效率极高，中国四小龙之一的最新基座",
    "paper": "MiniMax Technical Report"
  },
  {
    "id": "deepseek_r1",
    "name": "DeepSeek-R1",
    "date": "2025-01-20",
    "value": 10.0,
    "cluster": "Reasoning / RLVF",
    "domain": "reasoning",
    "conditions": "Pure RL (GRPO) without SFT Cold-start",
    "result": "开源推理能力 SOTA，证明了纯 RL 可激发 CoT 能力，对标 o1",
    "paper": "DeepSeek-R1: Incentivizing Reasoning Capability via RL",
    "experimentDetails": {
      "hardware": "大规模GPU集群",
      "gpuCount": 8000,
      "gpuModel": "H100",
      "trainingTime": "数月",
      "totalHours": "约12000小时",
      "tokens": "18T tokens",
      "dataSize": "18万亿tokens (推理数据)",
      "modelSize": "~67B",
      "learningRate": "4e-5",
      "batchSize": "3072",
      "cost": "约$50M (成本极低)"
    },
    "metrics": {
      "benchmarks": {
        "MATH": "93.2%",
        "HumanEval": "90.1%",
        "IMO": "58.3%",
        "AIME": "Top 1%",
        "GSM8K": "97.5%"
      },
      "latency": "~4-12s (推理时间)",
      "throughput": "~10 tokens/s",
      "reasoningSteps": "平均12-25步",
      "tokensPerDollar": "~8.5M tokens/$"
    }
  },
  {
    "id": "o3_mini",
    "name": "OpenAI o3-mini",
    "date": "2025-01-31",
    "value": 10.1,
    "cluster": "Reasoning / RLVF",
    "domain": "reasoning",
    "conditions": "Advanced RL Search + Reasoning",
    "result": "OpenAI 针对 R1 的回应，推理能力极强且速度更快",
    "paper": "OpenAI o3-mini Announcement",
    "paperUrl": "https://openai.com/zh-Hant/index/introducing-o3-and-o4-mini/",
    "githubUrl": "",
    "huggingfaceUrl": "",
    "experimentDetails": {
      "hardware": "大规模GPU集群",
      "gpuCount": 12000,
      "gpuModel": "H100",
      "trainingTime": "数月",
      "totalHours": "约18000小时",
      "tokens": "25T tokens",
      "dataSize": "25万亿tokens (推理数据)",
      "modelSize": "~80B",
      "learningRate": "2e-5",
      "batchSize": "4096",
      "cost": "约$250M"
    },
    "metrics": {
      "benchmarks": {
        "MATH": "96.8%",
        "HumanEval": "94.5%",
        "IMO": "68.2%",
        "USACO": "Platinum Level",
        "AIME": "Top 0.5%"
      },
      "latency": "~3-10s (推理时间)",
      "throughput": "~15 tokens/s",
      "reasoningSteps": "平均20-40步"
    }
  },
  {
    "id": "spark_x15",
    "name": "Spark X1.5",
    "date": "2025-10-01",
    "value": 8.8,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "MoE architecture + domestic hardware optimization",
    "result": "国产硬件上优化，数学推理能力强，综合性能达GPT-5的95%[5](@ref)",
    "paper": "Spark X1.5 Technical Report"
  },
  {
    "id": "qwen3",
    "name": "Qwen3",
    "date": "2025-08-01",
    "value": 9.2,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Massive scale training + DPO",
    "result": "中文优化极佳，多模态支持，在开源排行榜中领先[3](@ref)",
    "paper": "Qwen3 Technical Report"
  },
  {
    "id": "glm4_6",
    "name": "GLM-4.6",
    "date": "2025-09-30",
    "value": 9.0,
    "cluster": "RLHF (传统派)",
    "domain": "reasoning",
    "conditions": "Agentic coding optimization",
    "result": "代码能力国内领先，对齐Claude Sonnet 4，国产芯片适配突破[7](@ref)",
    "paper": "GLM-4.6 Technical Report"
  },
  {
    "id": "minimax_m2",
    "name": "MiniMax M2",
    "date": "2025-09-15",
    "value": 9.0,
    "cluster": "Direct Alignment (去RL化)",
    "domain": "general",
    "conditions": "Cost-effective training",
    "result": "开源性价比之王，在有限预算下性能卓越，登顶11月开源榜单[3](@ref)",
    "paper": "MiniMax M2 Technical Report"
  },
  {
    "id": "deepseek_v32_exp_thinking",
    "name": "DeepSeek-V3.2-Exp-Thinking",
    "date": "2025-10-10",
    "value": 9.1,
    "cluster": "Reasoning / RLVF",
    "domain": "reasoning",
    "conditions": "Reasoning-focused alignment",
    "result": "11月开源模型综合第一，数学推理和代码生成能力突出[6](@ref)",
    "paper": "DeepSeek-V3.2 Technical Report"
  },
  {
    "id": "longcat_flash_omni",
    "name": "LongCat-Flash-Omni",
    "date": "2025-11-03",
    "value": 9.0,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "Efficient multimodal architecture + real-time interaction",
    "result": "开源全模态SOTA，实现低延迟实时音视频交互，参数5600亿[1,4](@ref)",
    "paper": "LongCat-Flash-Omni Technical Report"
  },
  {
    "id": "ernie5",
    "name": "Ernie 5.0",
    "date": "2025-11-13",
    "value": 9.2,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "Native multimodal architecture + RLHF",
    "result": "原生全模态模型，多模态理解领先，国内综合性能第一[7](@ref)",
    "paper": "Ernie 5.0 Technical Report"
  },
  {
    "id": "claude4_5",
    "name": "Claude 4.5",
    "date": "2025-10-20",
    "value": 9.4,
    "cluster": "RLAIF (AI反馈)",
    "domain": "reasoning",
    "conditions": "Advanced RLAIF + Constitutional AI",
    "result": "安全性和长文本处理优势明显，推理能力进一步提升[5](@ref)",
    "paper": "Claude 4.5 Model Card"
  },
  {
    "id": "gpt5_1",
    "name": "GPT-5.1",
    "date": "2025-10-01",
    "value": 9.6,
    "cluster": "RLHF (传统派)",
    "domain": "general",
    "conditions": "Multi-model routing + adaptive reasoning",
    "result": "11月综合基准测试夺冠，数学推理和代码生成领先[5,6](@ref)",
    "paper": "GPT-5.1 Technical Report"
  }
]